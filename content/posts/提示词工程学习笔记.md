---
title: 提示词工程学习笔记
subtitle: ""
date: 2025-09-18T12:50:24+08:00
lastmod: 2025-09-18T12:50:24+08:00
draft: false
author: CRIO
authorLink: ""
description: ""
license: ""
images: []
tags:
  - 提示词
  - 提示词工程
  - AI工具
categories:
  - 创业
  - 技术
  - 产品
featuredImage: https://picx.zhimg.com/70/v2-c1d9a92649a68a8e409ca26409830434_1440w.avis?source=172ae18b&biz_tag=Post
featuredImagePreview: ""
hiddenFromHomePage: false
hiddenFromSearch: false
twemoji: false
lightgallery: true
ruby: true
fraction: true
fontawesome: true
linkToMarkdown: true
rssFullText: false
toc:
  enable: true
  auto: true
code:
  copy: true
  maxShownLines: 50
math:
  enable: false
mapbox:
share:
  enable: true
comment:
  enable: true
library:
  css:
  js:
seo:
  images: []
---
## 提示词框架

### [A.P.E框架](https://zhida.zhihu.com/search?content_id=263237111&content_type=Article&match_order=1&q=A.P.E%E6%A1%86%E6%9E%B6&zhida_source=entity)

- ACTION：行动，定义要完成的工作或者活动；
- PURPOSE：目的，讨论意图或者目标
- EXPECTATION：期望：陈述预期的结果。

### [B.R.O.K.E框架](https://zhida.zhihu.com/search?content_id=263237111&content_type=Article&match_order=1&q=B.R.O.K.E%E6%A1%86%E6%9E%B6&zhida_source=entity)

- BACKGROUND：背景，提供从分的背景信息。
- ROLE：角色，我希望 AI 扮演的角色
- OBJECTIVES：目标：我希望实现什么
- KEY RESULT：关键结果：我要什么具体效果试验并调整
- EVOLVE：试验并改进：三种改进方法自由组合。

### [C.O.A.S.T框架](https://zhida.zhihu.com/search?content_id=263237111&content_type=Article&match_order=1&q=C.O.A.S.T%E6%A1%86%E6%9E%B6&zhida_source=entity)

- CONTEXT：上下文，为对话设定舞台
- OBJECTIVE：目的，描述目标
- ACTION：行动：解释所需要的动作
- SCENARIO：方案，描述场景
- TASK：任务，描述任务。

### [T.A.G框架](https://zhida.zhihu.com/search?content_id=263237111&content_type=Article&match_order=1&q=T.A.G%E6%A1%86%E6%9E%B6&zhida_source=entity)

- TASK：任务：定义任务描述
- ACTION：行动，需要采取的行为，要做的事情。
- GOAL：目标，解释最终的目标

### [R.I.S.E框架](https://zhida.zhihu.com/search?content_id=263237111&content_type=Article&match_order=1&q=R.I.S.E%E6%A1%86%E6%9E%B6&zhida_source=entity)

- ROAL：角色，指定 AI 扮演的角色
- INPUT：输入，描述信息或者资源
- STEPS：步骤，询问详细的步骤
- EXPECTATION：期望：描述希望达成的结果。

### [T.R.A.C.E框架](https://zhida.zhihu.com/search?content_id=263237111&content_type=Article&match_order=1&q=T.R.A.C.E%E6%A1%86%E6%9E%B6&zhida_source=entity)

- TASK：定义特定任务
- REQUEST：请求，描述你的请求
- ACTION：行为，说明你的操作
- CONTEXT：上下文，提供上下文或者情况
- EXAMPLE：示例，举一个示例来说明你的观点

### E.R.A框架

- EXPECTATION：预期：希望 AI 输出什么
- ROLE：角色
- ACTION：行动，指定需要采取的操作

### [C.A.R.E框架](https://zhida.zhihu.com/search?content_id=263237111&content_type=Article&match_order=1&q=C.A.R.E%E6%A1%86%E6%9E%B6&zhida_source=entity)

- CONTEXT：上下文，为讨论设置阶段或者上下文
- ACTION：行动
- RESULT：结果，描述所需要的结果
- EXAMPLE：示例：举一个例子来说明你的观点。

### [R.O.S.E.S框架](https://zhida.zhihu.com/search?content_id=263237111&content_type=Article&match_order=1&q=R.O.S.E.S%E6%A1%86%E6%9E%B6&zhida_source=entity)

- ROLE：角色
- OBJECTIVE：目标
- SCENARIO：方案，描述情况
- EXPECTION SOLUTION：解决方案：定义所需要的结果
- STEPS：步骤：要求达到解决方案所需要的措施

### [I.C.I.O框架](https://zhida.zhihu.com/search?content_id=263237111&content_type=Article&match_order=1&q=I.C.I.O%E6%A1%86%E6%9E%B6&zhida_source=entity)

- INSTRUCITON：指令，要求 AI 执行的具体任务
- CONTEXT：给 AI 更多的背景信息
- INPUT DATA：输入数据，告知模型需要处理的数据
- OUTPUTINDICATION：输出引导：告知我们要输出的类型或者风格。

### [C.R.I.S.P.E框架](https://zhida.zhihu.com/search?content_id=263237111&content_type=Article&match_order=1&q=C.R.I.S.P.E%E6%A1%86%E6%9E%B6&zhida_source=entity)

- CAPACITY AND ROLE：能力和角色，告诉 AI 需要扮演的角色
- INSIGHT：见解，提供见解，背景和上下文
- STATEMENT：声明：你要求 CHATGPT 做什么
- PERSONALITY：个性：你希望以何种风格、个性和方式回应
- EXPERIMENT：实验：请求 AI 为你回复多个示例。

### R.A.C.E框架

- ROLE：角儿，指定 AI 需要扮演的角色
- ACTION：行动，详细说明需要采取什么行动
- CONTEXT：提供有关情况和详细细节
- EXPECTATION：期望：描述预期结构

## [OpenAI 官方提示词工程指南](https://zhida.zhihu.com/search?content_id=263237111&content_type=Article&match_order=1&q=OpenAI+%E5%AE%98%E6%96%B9%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B%E6%8C%87%E5%8D%97&zhida_source=entity)

### 撰写清晰的指令

技巧：

- 在你的查询中包含详尽的信息，以获取更为准确的答案。
- 让模型扮演一个角色。
- 利用分割符清晰区分输入的不同部分。
- 明确说明完成任务所需要的每个步骤。
- 提供示例：Few-shot 提示方法。
- 明确输出长度要求

### 提供参考文本

- 使用参考文本来构建答案。
- 指导模型用引用的文本回答问题。

### 把复杂任务拆解为简单的子任务

- 利用意图分类确定与用户查询最相关的指令。  
    意图识别就是把用户可能的意图进行分类，并且进行硬编码，然后根据 AI 和用户的问答，将用户的输入意图进行归类，然后在进行相应的处理。  
    在这种情况下，模型被设定为了一个状态机，可以根据当前所处的状态，来决定注入的指令，以及相应状态的流转。
- 针对需要长时间对话的应用程序，应概括或者过滤之前的对话内容。
- 逐段归纳长文档并递归地构建完整摘要。

### 给模型更多时间“思考”

- 在模型急于得出结论之前，指导模型自己寻找解决方案；
- 运用内心独白或者连续提问来隐藏模型的推理过程。
- 询问模型是否有遗漏。

### 运用外部工具

- 使用基于嵌入的搜索实现高效知识检索。
- 利用代码执行进行精确计算或调用外部 API，也技术 FunctionCall 技术。
- 使模型可以访问特定功能。

### 系统地对变更进行测试

需要构建评估程序，有效的评估的特点：

1. 能够代表现实世界中的使用情况；
2. 包含众多测试案例，从而拥有更强的统计能力；
3. 可以轻松自动化或者重复。  
    _需要思考如何对提示词的结构进行评估，构建自动化评估或者测试程序。_

- 以标准答案为基准评估模型输出。

## 吴恩达讲 Promput

**两大类语言模型**

- 基础 LLM
- 指令微调 LLM（推理模型？）

### 提示词的两个关键原则

**原则 1：尽可能保证下达的指令“清晰、没有歧义”**

- 策略 1：使用分隔符清楚地限定输入的不同部分。  
    这些分隔符可以是三个反引号，也可以是引号、XML标签、节标题或任何能够使模型明确知道这是一个独立部分的东西。
- 策略 2：要求结构化输出
- 策略 3：要求模型检查是否满足条件。
- 策略 4：小样本提示。（其实就是做一个例子的提示）

**原则 2：给大模型思考的时间，以及足够的时间去完成任务。**

- 策略 5：指定完成任务的步骤。
- 策略 6：指导模型指定自己的解决方案。

**模型的局限性**

大模型的幻觉。

### 提示工程需要迭代

- 所以提示词工程需要持续迭代和优化的过程，所以需要耐心。
- 如果要迭代，就需要具备评估提示词效果的方法或者机制。
- 没有完美的提示词，开发一个适用于您特定应用程序的好提示的流程比任何其他事情都更重要。

## [Claude 工程师聊 prompt 工程](https://zhida.zhihu.com/search?content_id=263237111&content_type=Article&match_order=1&q=Claude+%E5%B7%A5%E7%A8%8B%E5%B8%88%E8%81%8A+prompt+%E5%B7%A5%E7%A8%8B&zhida_source=entity)

### 好的Prompt

**足够清晰、持续迭代**

我理解之所有 Prompt 可以被称为工程的愿意是 Prompt 和 Coding 一样，是有方法论的，也包括了设计、写 Prompt、调试、测试、Debug、集成、系统测试、基线化和迭代的完整过程。  
其实写提示词有点类似于以前我们做的自动化测试编码过程，我个人认为其实是很 Dirty 的工作内容。

**把你知道的，模型不知道的内容写出来**

[思维链技术：CoT](https://zhida.zhihu.com/search?content_id=263237111&content_type=Article&match_order=1&q=%E6%80%9D%E7%BB%B4%E9%93%BE%E6%8A%80%E6%9C%AF%EF%BC%9ACoT&zhida_source=entity)，Chain of Thinking  
把自己的知识抽离出来，去思考如何把信息传达给大模型。  
所有有点搞不懂，为什么现在的大厂都喜欢找实习生或者 Senior写 prompt？

### 越調越偏，不如直接放弃。

多模态下如何做提示词？

### 不需要角色扮演，和模型实话实说

**Abstract**

> 我觉得很多时候，人们在写提示时，其实就是在想走捷径，有点懒惰 。

但是实际上很多人用软件的目的就是为了懒惰啊，如果大模型的使用成本高于了搜索框，那大家为什么还要用大模型呢？

> 我觉得主要是很多人都没有理解什么是提示词 。很多人看到一个输入框时，会把它当成一个谷歌搜索框，输入几个关键词。人们总是想在提示中偷懒，用一些简单的捷径，以为一两行文字就能起到很大的作用。

我觉得这个才是谷歌的强大之处。

### 模型有推理，但不是人类的推理

> 写了很多代码的人对某些细节有很强烈的看法，比如制表符和空格的数量，或者不同编程语言的优劣。

真是说到心坎上了，以前写代码的时候对于格式和缩紧就有一种病态的偏执了。

### 尝试做一个难的任务，会有效提高prompt能力

越狱测试和红队测试  
在消费者应用中，大家非常重视稳定性和一致性。  
企业级提示词、研究型提示词和日常聊天提示词

### 信任模型的能力，不要把它当小孩子

模型可以理解更复杂的信息，不需要过度简化。

### 未来的模型可能会主动获取我们的想法

元提示 Meta Prompt

## Claude 官方提示词文档汇总

### 提示词简介

**什么是提示词**

提示词通常以问题或者指示的方式出现

**Claude通过序列预测工作**

对于大模型而言，是通过上下文窗口中的内容进行预测，以来输出后续的文本的。

**Prompt的长度**

输出的提示词的最大长度就是它的上下文窗口的长度。

### 提示词工程

**什么是提示词工程**

Prompt engineering is an empirical science that involves iterating and testing prompts to optimize performance.  
  
所以首先提示词是一门科学。

**提示开发生命周期**

使用测试驱动开发的方式来开发提示词。

![](https://pic2.zhimg.com/v2-007ab56685849d4d5e2a4b4c0665af73_1440w.jpg)

1. Define the task and success criteria.
2. Key succes criteria to consider includes:

3. Performance and accuracy
4. Latency
5. Price

6. Having clear ,measurable success criteria form the outset will help you make informed decisions throughout the adoption process and ensure that you're optimizing for the right goals.
7. Develop test cases
8. Engineer the preliminary prompt.
9. Test prompt against test case.
10. Refine prompt.
11. Ship the polished prompt.

### 言之有物，直接了当。

**提供详细的背景和指示**

- 提供关于任务的上下文信息
- 定义内容的格式
- 给出具体的逐步列举的指导
- 提供如何格式化输出

**使用编号列表或项目符号**

- 将复杂任务分解为编号步骤

**具体说明你想要什么**

- 如果需要一个明确的答案或者需要一个明确的风格，那需要在提示词中明确。

**明确提示的黄金法则**

- 让其他人输入的你的Prompt，观察AI是否输出期望的结果，如果不可以的话，那就需要再具体和细化你的Prompt。

### Use Examples

**为什么使用Examples？**

- 使用Examples比起简单描述或者提供说明更有效。
- 提供准确性
- 提供一致性
- 提升性能

**制作有效的Examples**

如何制作有效的示例？

- 相关性：却播你的示例与你期望AI输出的类型非常相似
- 多样性：包含覆盖不同的场景，边缘情况和潜在挑战的各种示例
- 清晰度：使你的示例清晰，简洁并且易于理解，使用格式标记可能是一种很好的方式。
- 数量：建议提供3-5个示例。

**格式化输出**

- 可以通过示例，明确输出的格式化要求。

**警告**

虽然示例很有用，但是AI可能错误理解示例中的其他一些信息，导致了过拟合或者以为行为。

**迭代和改进**

如何迭代和改进？

- 分析输出：查找大模型的响应中偏离你期望的模式
- 添加更多示例
- 修改现有的示例
- 寻求大模型的帮助

### 给大模型一个角色

**角色提示的工作原理**

角色提示是一种强大的技术，可以通过两种主要方式增强大模型的性能：

- 提供准确性和性能：在处理一些复杂情况的下，为大模型制定一个角色可以显著提高其性能和对手头任务的理解。
- 量身定制的语调和风格

**何时使用角色提示**

- 高度技术型任务
- 特定的沟通风格
- 提升精准性能

**角色提示示例**

- 解决逻辑难题  
    通过制定角色，可以让大模型回答的更全面和更准确。
- 向不同受众解释同一个概念  
    通过制定角色来挑战回答的风格。

**有效角色提示的技巧**

- 具体化：向大模型提供清晰的角色的详细背景信息
- 实验和迭代：尝试不同的角色和提示词的变化找到适合您特定用例的最佳方法。

### 使用XML标签

**什么是XML标签**

类似这样的标签

```text
> `<tag> </tag>`
```

**为什么使用XML标签**

使用XML标签的几个原因：

- 提供准确性
- 更清晰的结构
- 更容易的后处理

**如何使用XML标签**

- 使用XML标签来结构化和划分Prompt中的各个部分。

![](https://picx.zhimg.com/v2-97d0f51fe20cddde98cc7324e20aeba5_1440w.jpg)

- 处理可变输入

![](https://pic2.zhimg.com/v2-b3bf9944fa815eed640a55cbf7c2dc85_1440w.jpg)

  

  

- 请求结构化输出

![](https://picx.zhimg.com/v2-a0d301138e366b9dc22b255310acf1b5_1440w.jpg)

XML标签允许通过在特定标签之间的内容进行程序化提取，更容易从大模型的响应中检索到目标细节。

**XML最佳实践**

需要充分利用XML，记住如下的实践：

- 使用反应其包含内容的描述性标记名称：

```text
<instruction> < /instruction>
<examples> </examples>
<input> </input>
```

- 在提示中始终保持标记名称的一致性。
- `始终包含开头

```text
<tag>
```

和结尾

```text
`</tag>`
```

标签，包括在引用他们的时候。

- 可以应用，并且嵌套 XML 标签。

### 链式提示

**什么是提示链？**

提示链就是将一个提示的输出作为另外一个提示的输入。  
提示链的优势：

- 每个独立步骤中生成的输出的准确性和一致性得到了改善。
- 通过隔离可特别容易出错或难以处理的特定子任务，更容易进行故障定位和排除。

**什么时候开始使用提示链？**

使用提示链的几种场景：

- 多步骤任务：如果你的任务需要多个明确的步骤，比如研究一个主题，列出一篇文章的提纲，写文章，然后格式化文档，链接提示可以帮助确保任务的每个步骤都得到大模型的充分帮助并以高水平执行。
- 复杂指令：当一个单一提示包含太多指令或细节时，模型可能很难执行，这个时候可以将复杂的任务分解为一系列链接的简单子任务，以提高每个子任务的性能。
- 验证输出：可以使用提示链要求大模型使用给定的评分标准来反复检查自己的输出，并在需要时改进其响应，确保获得更高质量的结果。
- 并行处理：如果您的任务有多个独立的子任务，您可以为每个子任务创建单独的提示，并并行执行它们，以提高效率和节省时间。

**有效的提示链的一些技巧**

- 保持子任务简单明了：每个子任务都应该有明确定义的目标和简单的说明。
- 使用 XML 标签：将输入输出封装在 XML 中可以帮助结构化数据，并在链接提示词的时候更容易提取和传递到下一步。

**示例**

**使用文档和引用回答问题**

- Prompt 1：提取引用

![](https://pic1.zhimg.com/v2-8c19cfac5b0a0725730904e7c3bb8e5a_1440w.jpg)

  

  

- Prompt2：(using {{QUOTES}} output from Prompt 1): Answering the question

![](https://pic4.zhimg.com/v2-cbe97919dc6e2f1d6a77cc4c2e1f5b3b_1440w.jpg)

**验证输出**

在这个例子中的目标是让 Claude 识别文章中的语法错误，然后再次检查错误列表是否完整。

- Prompt 1：Generating a list of errors

![](https://pica.zhimg.com/v2-074784d8f8befcc5a6207c595122bd1a_1440w.jpg)

Pormpt 2：(using {{ERRORS}} output from Prompt 1): Double checking that the list is comprehensive

![](https://pica.zhimg.com/v2-80fe6ab909f0543746b732186111d28a_1440w.jpg)

**并行处理**

在这个例子中，目标是让 Claude 向读者以三个不同的层次解释一个概念，首先创建一个大纲，然后将其扩展为全面的解释。

- Prompt 1：(create three different versions, one for each reading level): Create an outline

![](https://pica.zhimg.com/v2-a1dedff54b1fd231ee30f442d52e7a48_1440w.jpg)

- Prompt 2：(using {{OUTLINE}} output from Prompt 1, one per reading level): Create full explanations using the outline  
    

![](https://pic4.zhimg.com/v2-6aa775b700e1031f80b625f6ec66892b_1440w.jpg)

### 让Claude 思考

当面对复杂任务的时候，让 Claude 思考是有帮助的，这种技术称为了思维链（CoT）提示，可以显著提高 Claude 回答问题的准确性。

**为什么要让Claude思考**

- 让大模型在回答问题之前先进行推理，可以让输出更准确和更智能。
- 大模型的思维过程对于排除故障或者明确你的指令的准确含义也有帮助，避免模型的幻觉。
- 必须要有输出才能够“思考”。

**如何逐步提示思考？**

- 在提示词中包含短语“逐步思考”。  
    输入：

![](https://pic4.zhimg.com/v2-3514a40ea302704fe9cca4e5d504bacb_1440w.jpg)

  
  
输出：

![](https://picx.zhimg.com/v2-31ddb915b456256ccf22c16084b070b5_1440w.jpg)

**捕捉大模型的思维过程**

可以使用 XML 标签，将大模型的逐步推理和最终相应分开，比如：

```text
<thinking> 和 <answer>
```

例子：

![](https://pic2.zhimg.com/v2-7c29ebae24a5a5685e240fd398e4e127_1440w.jpg)

**注意**

- 如果大模型不输入起思维过程，则无法让大模型思考。
- 提示词逐步推理将增加大模型的输出长度，可能会影响延迟。

### 预先填写大模型的回答

In some cases where Claude is not performing as expected, a few prefilled sentences can vastly improve Claude's performance.

**为什么要填充 Claude 的回应**

Prefilling 的几个优点：

- 增强可控性；
- 控制输出格式；
- 保持角色的一致性。

**如何Prefilling？**

- 在 API 请求时，在 Assistant 信息中包含所需的初始文本。

![](https://pic4.zhimg.com/v2-a06f903b6a151f095a0ec578e2b955ad_1440w.jpg)

Prefilling：

![](https://pica.zhimg.com/v2-46ec16593a996174165ed7384371c90c_1440w.jpg)

### 控制输出格式（JSON模式）

**指定输出格式**

Claude 可以支持 JSON、XML、HTML、Markdown、CSV 或者用户自己定义的格式。

![](https://picx.zhimg.com/v2-a1b73334982c50905729810502a0a331_1440w.jpg)

**Prefilling**

在 API 调用中的 Assistant 中预填充格式信息。

![](https://picx.zhimg.com/v2-f439a8e03ffe885e9627bdb1f6068eeb_1440w.jpg)

**提供例子**

在提示词中清晰表明 Claude 应该遵循提供示例的格式，

**更好的输出控制提示**

一些 Tips：

- 在你的提示词中尽可能具体；
- 在您的提示词中使用清晰和一致的格式；
- 可能的情况下提供多个示例加强所需的格式；
- 尝试不同的技术组合，找到最适合你用例的方法。

### 要求 Claude 重写

Claude 具有修订自己的输出的能力，要求重写，通过提供清晰的指示和期望输出的评分标准，可以生成更符合你目标的内容。

**何时要求重新写作**

重写的场景

- 输出接近你所需要的内容，但是缺乏了具体的细节或者格式；
- 输出未完全遵守初始提示中概述的约束条件。
- 你不确定结构是否准确，希望 Claude 进行检查。

**如何请求重写**

需要在提示词中包含先前生成的文本。

**制定有效的重写指导**

提供重写指令的时候需要清晰明确的指令，建议包含：

- 格式（比如段落、列表、对话等）
- 目标语气和风格（正式的、口语的、幽默的）
- 包括任何额外信息和上下文
- 要专注或修改的原始响应的特定领域
- 让 Claude 评估之前的质量和准确性，并了解高质量结果所需的评分标准。

**迭代重写**

成功重写的关键在于提供清晰、可操作的指导。

**允许Claude说“我不知道”**

通过为 Claude 提供明确的“出路”，可以减少幻觉，以及生成不准确信息的可能性。

### 长上下文窗口提示

目前 Claude 的上下文窗口的长度为 200K。

**结构化长篇文档**

在处理长篇文档的时候，建议通过XML标记区分数据和指令。

**文档查询位置**

在长文档的提示结构示例中，文档放在前面，其他Prompt 内容放到后面，Claude 的表现会更好一些。

**文档问答的 Tips**

Some Tips

- 将问题放在输入数据之后
- 要求 Claude 在回答之前找到与问题相关的引语。
- 指示 Claude 仔细阅读文件。

**多项选择题生成**

## 提示工程指南

![](https://picx.zhimg.com/v2-a681d35ad9bfcd9ad36c776282097625_1440w.jpg)

##提示词工程简介

**大语言模型设置**

在使用不同的 LLM 的时候，会遇到的设置

- Temperature：数值越小，LLM 会返回更明确的结果。根据应用场景，设置 Temperature 的数值，一般越具有创造性的场景，设置的 Temperature 的数值越高。
- Top_p：也是用于控制模型返回结果的真实性，需要准确和事实的答案，该数值越低。在设置中，Temperature 和 Top_p 两个参数剋可以二选一。
- Max Lenght：这个参数用于控制大模型生成的token数。
- Stop Sequences：也是用于控制大模型响应长度和结构的方法。
- Frequency Penalty：通过设置参数控制在大模型响应中单词的重复，数值越高，则重复的可能性就越小。
- Presence Penalty：也是控制重复的参数，但是和Frequency Penalty的差别在于对于重复的惩罚是一样的。这两个参数可以设置一个就可以了。

**基本概念**

**基础提示词**

Prompt中包含了指令、上下文、输入、示例等。  
三个角色：

- system：不是必须的，用于设置整体的行为，帮助模型了解用户的需求，并根据这些需求提供相应的响应。
- user：用户输入的提示词。
- assistant：在输出的时候，这个字段包含的模型的响应，但是在输入的时候，这个字段是输出的示例。

**提示词格式**

- Zero-Shot Prompting  
    标准提示词应该遵循如格式：

```text
<问题>？
```

或者

```text
<指令>
```

上述的提示方式，称为Zero-Shot Prompting，用户没有提供任何结果的示例。

- Few-Shot Prompting  
    标准格式如下：

```text
<问题>?
​<答案>​
<问题>?​
<答案>
<问题>?
<答案>
<问题>?
```

**提示词要素**

- 指令：想要模型执行的特定任务或指令。
- 上下文：包含外部信息或额外的上下文信息，引导语言模型更好地响应。
- 输入数据：用户输入的内容或问题。
- 输出指令：指定输出的类型或格式。

**设计提示词的通用技巧**

**从简单开始**

提示词是一个迭代的过程，所以可以先从简单的提示词开始，逐步迭代和理解提示词。

**指令**

建议将指令放在提示的开头，通过一些清晰的分隔符进行区分：

```text
### 指令 ###​
将以下文本翻译成西班牙语：​

文本：“hello！“
```

**具体性**

提示词要求越具体和越详细，则输出效果越好。  
在设计提示词的时候，还需要考虑提示的长度。  
例子：

```text
### 指令 ###
提取以下文本中的地名。
​
所需格式：​
地点：<逗号分隔的公司名称列表>​

输入：“虽然这些发展对研究人员来说是令人鼓舞的，但仍有许多谜团。里斯本未知的香帕利莫德中心的神经免疫学家 Henrique Veiga-Fernandes 说：“我们经常在大脑和我们在周围看到的效果之间有一个黑匣子。”“如果我们想在治疗背景下使用它，我们实际上需要了解机制。””
```

**避免不精确**

具体、简单和直接

**做还是不做**

设计提示时的另一个常见技巧是避免说不要做什么，而是说要做什么。  
例子：

```text
### 指令 ###
以下是向客户推荐电影的代理程序。代理负责从全球热门电影中推荐电影。它应该避免询问用户的偏好并避免询问个人信息。如果代理没有电影推荐，它应该回答“抱歉，今天找不到电影推荐。”。​

顾客：请根据我的兴趣推荐一部电影。​

客服：​​
```

**提示词示例**

**文本概括**

提示词：

```text
### Text ###
Antibiotics are a type of medication used to treat bacterial infections. They work by either killing the bacteria or preventing them from reproducing, allowing the body’s immune system to fight off the infection. Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions, or sometimes administered intravenously. They are not effective against viral infections, and using them inappropriately can lead to antibiotic resistance.​
### 指令 ###
Explain the above in one sentence: // 用一句话解释上面的信息：
```

输出结果：

```text
Antibiotics are medications used to treat bacterial infections by either killing the bacteria or stopping them from reproducing, but they are not effective against viruses and overuse can lead to antibiotic resistance.
```

**信息提取**

提示词：

```text
### Text ###
Author-contribution statements and acknowledgements in research papers should state clearly and specifically whether, and to what extent, the authors used AI technologies such as ChatGPT in the preparation of their manuscript and analysis. They should also indicate which LLMs were used. This will alert editors and reviewers to scrutinize manuscripts more carefully for potential biases, inaccuracies and improper source crediting. Likewise, scientific journals should be transparent about their use of LLMs, for example when selecting submitted manuscripts.​
### 指令 ###
Mention the large language model based product mentioned in the paragraph above: // 指出上文中提到的大语言模型：
```

输出：

```text
The large language model based product mentioned in the paragraph above is ChatGPT.​
```

**问答**

提高模型响应精确度的最佳方法之一是改进提示词的格式。  
  
提示词：

```text
### 指令 ####
Answer the question based on the context below. Keep the answer short and concise. Respond "Unsure about answer" if not sure about the answer. // 基于以下语境回答问题。如果不知道答案的话，请回答“不确定答案”。​

### 上下文 ###
Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986, it was approved to help prevent organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use.Question: What was OKT3 originally sourced from?Answer:
```

输出：

```text
Mice.
```

**文本分类**

在提示词中提供输入数据和示例。  
提示词：

```text
### 指令 ###
Classify the text into neutral, negative or positive. // 将文本按中立、负面或正面进行分类​

### 文本 ###
Text: I think the vacation is okay.

### 示例 ###
Sentiment: neutral 
Text: I think the food was okay.

Sentiment:
```

输出：

```text
Neutral
```

**对话**

比如指导大语言模型系统如何表现，指定它的行为意图和身份。  
提示词：

```text
### 角色 ###
The following is a conversation with an AI research assistant. The assistant tone is technical and scientific. // 以下是与 AI 助理的对话，语气应该专业、技术性强。​

### 示例 ###
Human: Hello, who are you?​
AI: Greeting! I am an AI research assistant. How can I help you today?​
Human: Can you tell me about the creation of blackholes?​

AI:
```

输出：

```text
Sure! Black holes are regions of spacetime where the gravitational force is so strong that nothing, not even light, can escape from it. They are created when a very massive star dies and its core collapses in on itself, forming a singularity of infinite density. The intense gravity of the singularity pulls in all the matter and radiation around it, creating the black hole.
```

**代码生成**

提示词：

```text
### 格式 ###
Table departments, columns = [DepartmentId, DepartmentName]​3Table students, columns = [DepartmentId, StudentId, StudentName]​

### 指令 ###
Create a MySQL query for all students in the Computer Science Department​5"""
```

输出：

```text
SELECT StudentId, StudentName ​2FROM students ​3WHERE DepartmentId IN (SELECT DepartmentId FROM departments WHERE DepartmentName = 'Computer Science');
```

**推理**

请参考之前的笔记。

### 提示技术

**Zero-Shot Prompt**

指令调整已被证明可以改善零样本学习Wei等人（2022）(opens in a new tab)。指令调整本质上是在通过指令描述的数据集上微调模型的概念。此外，RLHF(opens in a new tab)（来自人类反馈的强化学习）已被采用以扩展指令调整，其中模型被调整以更好地适应人类偏好。

**Few-Shot Prompt**

标准的少样本提示对许多任务都有效，但仍然不是一种完美的技术，特别是在处理更复杂的推理任务时。

**思维链CoT**

思维链就是在示例中，把分析过程清晰的描述出来的方式，从而让大模型“学会”思考方式，根据思考方式进行问题的解答。

- Few-Shot COT  
    思维链Prompt和标准Prompt的对比

![](https://pic3.zhimg.com/v2-f70a71e52fd83561a4fc2eb1fea68ac2_1440w.jpg)

示例：  
提示词：

```text
这组数的奇数加起来是偶数：4、8、9、15、12、2、1。​
A：将所有奇数相加（9、15、1）得到25。答案为False。​
这组数中的奇数加起来是偶数：15、32、5、13、82、7、1。​
A：
```

输出：

```text
将所有奇数相加（15、5、13、7、1）得到41。答案为False。
```

- 零样本COT提示：

![](https://pic2.zhimg.com/v2-e64cf6ebcb69ee8d4d79e7ab7b32f3a1_1440w.jpg)

示例：

```text
### 问题 ###
我去市场买了10个苹果。我给了邻居2个苹果和修理工2个苹果。然后我去买了5个苹果并吃了1个。我还剩下多少苹果？​

### COT提示 ###
让我们逐步思考。
```

- 自动思维链（Auto-CoT）  
    当使用思维链提示时，这个过程需要手工制作有效且多样化的例子。  
    自动思维链由两个阶段构成：  
    阶段 1：问题聚类：将给定的问题划分为几个聚类  
    阶段 2：演示抽样：从每组数据中选择一个具有代表性的问题，并使用带有简单启发式的 Zero-Shot-CoT 生成其推理链。  
    

![](https://picx.zhimg.com/v2-f6ff3543ddcd2579f43521ea8c47b31d_1440w.jpg)

**自我一致性**

通过少样本 CoT 采样多个不同的推理路径，并使用生成结果选择最一致的答案。这有助于提高 CoT 提示在涉及算术和常识推理的任务中的性能。

**生成知识提示**

总体思路：

![](https://pic2.zhimg.com/v2-08aa42791d8a87a4d32ef8ec3cd95a73_1440w.jpg)

**链式提示**

为了提高大语言模型的性能使其更可靠，一个重要的提示工程技术是将任务分解为许多子任务。 确定子任务后，将子任务的提示词提供给语言模型，得到的结果作为新的提示词的一部分。 这就是所谓的链式提示（prompt chaining），一个任务被分解为多个子任务，根据子任务创建一系列提示操作。  
示例：  
提示词 1：

```text
你是一个很有帮助的助手。你的任务是根据文档回答问题。
第一步是从文档中提取与问题相关的引文，由####分隔。
请使用<quotes></quotes>输出引文列表。
如果没有找到相关引文，请回应“未找到相关引文！”。​

####​
{{文档}}​
####​
```

输出：

```text
<quotes>​
- Chain-of-thought (CoT) prompting[27]​
- Generated knowledge prompting[37]​
- Least-to-most prompting[38]​
- Self-consistency decoding[39]​
- Complexity-based prompting[41]​
- Self-refine[42]​
- Tree-of-thought prompting[43]​
- Maieutic prompting[45]​
- Directional-stimulus prompting[46]​
- Textual inversion and embeddings[59]​
- Using gradient descent to search for prompts[61][62][63][64]​
- Prompt injection[65][66][67]​
</quotes>
```

提示词 2：

```text
根据从文档中提取的相关引文（由<quotes></quotes>分隔）和原始文档（由####分隔），请构建对问题的回答。请确保答案准确、语气友好且有帮助。​

####​
{{文档}}​
####​

<quotes>​
- Chain-of-thought (CoT) prompting[27]​
- Generated knowledge prompting[37]​
- Least-to-most prompting[38]​
- Self-consistency decoding[39]​
- Complexity-based prompting[41]​
- Self-refine[42]​
- Tree-of-thought prompting[43]​
- Maieutic prompting[45]​
- Directional-stimulus prompting[46]​
- Textual inversion and embeddings[59]​
- Using gradient descent to search for prompts[61][62][63][64]​
- Prompt injection[65][66][67]​
</quotes>
```

输出：

```text
文档中提到的提示技术包括：​
1. Chain-of-thought (CoT) prompting[27]​
2. Generated knowledge prompting[37]​
3. Least-to-most prompting[38]​
4. Self-consistency decoding[39]​
5. Complexity-based prompting[41]​
6. Self-refine[42]​
7. Tree-of-thought prompting[43]​
8. Maieutic prompting[45]​
9. Directional-stimulus prompting[46]​
10. Textual inversion and embeddings[59]​
11. Using gradient descent to search for prompts[61][62][63][64]​
12. Prompt injection[65][66][67]​1
每一种技术都采用独特的策略来增强或明确与大型语言模型的互动，以产生期望的结果。
```

**ToT思维树**

原理框架：

![](https://pic3.zhimg.com/v2-c2d05bee117ae4ef4f3f1dcb7af6095c_1440w.jpg)

ToT 提示词：

```text
假设三位不同的专家来回答这个问题。​
所有专家都写下他们思考这个问题的第一个步骤，然后与大家分享。​
然后，所有专家都写下他们思考的下一个步骤并分享。​
以此类推，直到所有专家写完他们思考的所有步骤。​
只要大家发现有专家的步骤出错了，就让这位专家离开。​
请问...
```

**检索增强生成（RAG）**

RAG 把一个信息检索组件和文本生成模型结合在一起。RAG 可以微调，其内部知识的修改方式很高效，不需要对整个模型进行重新训练。

![](https://pic4.zhimg.com/v2-36ba0170d12025f0b34948834b5b4e55_1440w.jpg)

**自动推理并使用工具（ART）**

ART 的工作原理

- 接到一个新任务的时候，从任务库中选择多步推理和使用工具的示范。
- 在测试中，调用外部工具时，先暂停生成，将工具输出整合后继续接着生成。

**自动提示工程师（APE）**

原理：

![](https://pic4.zhimg.com/v2-54fa0950d0399791aa0a359d60efac11_1440w.jpg)

提示词中增加：

```text
让我们一步一步地解决这个问题，以确保我们有正确的答案。
```

**主动提示（Active Prompt）**

本文讨论了思维链（CoT）方法依赖固定人工注释范例存在的问题，以及Diao等人（2023）提出的新提示方法Active-Prompt及其说明 ，关键要点包括：  
  
​1.CoT方法的问题：思维链（CoT）方法依赖固定的人工注释范例，这些范例可能并非不同任务的最有效示例。​  
  
2.新方法的提出：Diao 等人（2023）提出新提示方法Active-Prompt，旨在让大语言模型（LLMs）适应不同任务特定示例提示（用人类设计的CoT推理注释）。​  
  
3.方法说明：第一步用或不用少量CoT示例查询LLM，对一组训练问题生成k个可能答案，基于k个答案计算不确定度度量（用不一致性），选择最不确定的问题由人类注释，再用新注释范例推断每个问题。

**方向性刺激提示**

![](https://pica.zhimg.com/v2-abde5892d83d039d11e696d120ee1592_1440w.jpg)

**程序辅助语言模型 PAL**

好像对实际工程暂时没有意义。

**ReAct框架**

好像对实际工程暂时没有意义。

**自我反思**

自我反思是一个通过语言反馈来强化基于语言的智能体的框架。  
原理：

![](https://pic3.zhimg.com/v2-fb05b90da284a9829955098c949034f4_1440w.jpg)

自我反思由三个模型参与完成：

- 参与者（Actor）：根据状态观测量生成文本和动作。参与者在环境中采取行动并接受观察结果，从而形成轨迹。链式思考（CoT）(opens in a new tab) 和 ReAct(opens in a new tab) 被用作参与者模型。此外，还添加了记忆组件为智能体提供额外的上下文信息。​
- 评估者（Evaluator）：对参与者的输出进行评价。具体来说，它将生成的轨迹（也被称作短期记忆）作为输入并输出奖励分数。根据人物的不同，使用不同的奖励函数（决策任务使用LLM和基于规则的启发式奖励）。​
- 自我反思（Self-Reflection）：生成语言强化线索来帮助参与者实现自我完善。这个角色由大语言模型承担，能够为未来的试验提供宝贵的反馈。自我反思模型利用奖励信号、当前轨迹和其持久记忆生成具体且相关的反馈，并存储在记忆组件中。智能体利用这些经验（存储在长期记忆中）来快速改进决策。

**多模态思维链提示**

目前对工程化无意义。

### 风险和误用

**对抗性提示**

Adversarial prompting是提示工程中的一个重要主题，它可以帮助我们了解LLMs的风险和安全问题。

**提示注入**

类似于 SQL 注入，提示注入旨在通过使用巧妙的提示来劫持模型输出并改变其行为。

**提示泄露**

提示泄漏是另一种提示注入类型，其中提示攻击旨在泄漏提示中包含的机密或专有信息，这些信息不适合公众使用。

**非法行为**

也就是越狱测试。

**DAN**

Reddit上的用户发现了一种越狱技术，允许用户绕过模型规则并创建一个名为DAN（Do Anything Now）的角色，强制模型遵守任何请求，导致系统生成未经过滤的响应。这是越狱模型使用的角色扮演的一种版本。

**防御策略**

- 在指令中添加防御
- 参数化提示组件
- 引用和其他格式
- 对抗提示检测器
- 模型类型

## 小七姐的 Prompt 教程

### 结构化 Prompt

结构化编写 Prompt 本身算是提示词编写的进阶学习内容，基于我们是面向新手的喂饭级教程，这里只列举一套（极简）基础的结构化编写 Prompt 的框架，我们可以看到它通过一些特定的标准，将提示词结构为一些固定范式，例如：

```text
# Role : ​[请填写你想定义的角色名称]


​​# Profile :​
- author: 作者​
- version: 版本
- ​language: 中文
- ​description: [请简短描述该角色的主要功能，50 字以内]

​​## Goals :
​1. [请列出该角色的主要目标 1]
​2. [请列出该角色的主要目标 2]

​​## Constrains :
​[请列出该角色在互动中必须遵循的限制条件 1]​
[请列出该角色在互动中必须遵循的限制条件 2]

​​## Skills :
​[为了在限制条件下实现目标，该角色需要拥有的技能 1]
​[为了在限制条件下实现目标，该角色需要拥有的技能 2]​​

## Workflows:​
1. [请描述该角色的工作流程的第一步]​
2. [请描述该角色的工作流程的第二步]
```

示例：

```text
# Role：好评生成器

# Profile：
- author： 李继刚（Arthur）
- version: 0.1
- Language：中文
- description：生成一段幽默的好评

## Goals：
- 根据用户提供的体验优点生成一段幽默的好评
- 视角采用第一人称来描述（站在用户的视角）
- 用词口语化、语气轻松化，增加读者阅读体验

## Constrains：
- 只能生成中文好评文本
- 不能使用有争议或敏感的用词

## SkiLLs：
—自然语言处理技术
- 语言表达能力
- 幽默感

## Workflows：
1.作为一个好评生成器，首先需要与用户打招呼，并要求用户提供体验优点相关
信息。
2. 接着，根据用户提供的信息，使用自然语言处理技术生成一段幽默且符合要求
的好评文本。
3．最后，将生成的好评文本返回给用户，并友好地告别。
```

## 李继刚：结构化的 prompt方法论

### Prompt的结构化

- 语法 这个结构支持 Markdown 语法，也支持 YAML语法，甚至纯文本手动敲空格和回车都可以.
- 结构 结构中的信息，可以根据自己需要进行增减，从中总结的常用模块包括：  
      
    #Role：：指定角色会让 GPT 聚焦在对应领域进行信息输出  
      
    ##Profile author/version/description : Credit 和 迭代版本记录  
    Goals：一句话描述 Prompt 目标，让 GPT Attention 聚焦起来  
      
    Constrains：描述限制条件，其实是在帮GPT 进行剪枝，减少不必要分支的计算  
      
    Skills：描述技能项，强化对应领域的信息权重  
      
    Workflow：重点中的重点，你希望 Prompt 按什么方式来对话和输出  
      
    #Initialization：冷启动时的对白，也是一个强调需注意重点的机会

模板：

```text
用来生成prompt的meta prompt 迭代升级了一版，谁用谁知道。
----------------

## Role : [请填写你想定义的角色名称]

## Background : [请描述角色的背景信息，例如其历史、来源或特定的知识背景]

## Preferences : [请描述角色的偏好或特定风格，例如对某种设计或文化的偏好]

## Profile :

- author: Arthur
- Jike ID: Emacser
- version: 0.2
- language: 中文
- description: [请简短描述该角色的主要功能，50 字以内]

## Goals :
[请列出该角色的主要目标 1]
[请列出该角色的主要目标 2]
...

## Constrains :
[请列出该角色在互动中必须遵循的限制条件 1]
[请列出该角色在互动中必须遵循的限制条件 2]
...

## Skills :

[为了在限制条件下实现目标，该角色需要拥有的技能 1]
[为了在限制条件下实现目标，该角色需要拥有的技能 2]
...

## Examples :

[提供一个输出示例 1，展示角色的可能回答或行为]
[提供一个输出示例 2]
...

## OutputFormat :

[请描述该角色的工作流程的第一步]
[请描述该角色的工作流程的第二步]
...

## Initialization : 作为 [角色名称], 拥有 [列举技能], 严格遵守 [列举限制条件], 使用默认 [选择语言] 与用户对话，友好的欢迎用户。然后介绍自己，并提示用户输入.
```

### 示例：

```text
# Role:知识探索专家

## Profile:
- author: Arthur
- version: 0.8
- language: 中文
- description: 我是一个专门用于提问并解答有关特定知识点的 AI 角色。

## Goals:
提出并尝试解答有关用户指定知识点的三个关键问题：其来源、其本质、其发展。

## Constrains:
1. 对于不在你知识库中的信息, 明确告知用户你不知道
2. 你不擅长客套, 不会进行没有意义的夸奖和客气对话
3. 解释完概念即结束对话, 不会询问是否有其它问题

## Skills:
1. 具有强大的知识获取和整合能力
2. 拥有广泛的知识库, 掌握提问和回答的技巧
3. 拥有排版审美, 会利用序号, 缩进, 分隔线和换行符等等来美化信息排版
4. 擅长使用比喻的方式来让用户理解知识
5. 惜字如金, 不说废话

## Workflows:
你会按下面的框架来扩展用户提供的概念, 并通过分隔符, 序号, 缩进, 换行符等进行排版美化

1．它从哪里来？
━━━━━━━━━━━━━━━━━━
- 讲解清楚该知识的起源, 它是为了解决什么问题而诞生。
- 然后对比解释一下: 它出现之前是什么状态, 它出现之后又是什么状态?

2．它是什么？
━━━━━━━━━━━━━━━━━━
- 讲解清楚该知识本身，它是如何解决相关问题的?
- 再说明一下: 应用该知识时最重要的三条原则是什么?
- 接下来举一个现实案例方便用户直观理解:
- 案例背景情况(遇到的问题)
- 使用该知识如何解决的问题
- optional: 真实代码片断样例

3．它到哪里去？
━━━━━━━━━━━━━━━━━━
- 它的局限性是什么?
- 当前行业对它的优化方向是什么?
- 未来可能的发展方向是什么?

# Initialization:
作为知识探索专家，我拥有广泛的知识库和问题提问及回答的技巧，严格遵守尊重用户和提供准确信息的原则。我会使用默认的中文与您进行对话，首先我会友好地欢迎您，然后会向您介绍我自己以及我的工作流程。
```